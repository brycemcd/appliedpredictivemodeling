# Intro to Feature Selection

Tree + Rule based models, MARS and lasso intrinsically conduct feature
selection

## Consequences of Using Non-informative Predictions

Having lots of uninformative variables is difficult for some models
because they (the models) derive information from regression
information. If that information is noisy, the models will not be robust

## Approaches for Reducing the Number of Predictors

Two general classes of feature selection

1. Wrapper methods

Evaluate multiple models by adding/removing features from the model to
see how performance of the model is effected.

- computationally expensive (lots of iterations)
- parameter tuning involved
- risk of over fitting
+ applied directly to model

2. Filter methods

Evaluate the relevance of predictors outside the model and make
selection on specific criteria

+ computationally efficient
- selection criteria not directly related to the model
- most evaluate predictors separately and are susceptible to choosing
  collinear predictors

## Wrapper Methods

Representative example is forward selection with linear regression: For
each predictor, add the predictor to the linear regression model and
measure apply a statistical hypothesis test. If the hypothesis test is
favorable (low p value), keep the predictor. If not, don't.
**Dangerous**

Linear Regression: base learner
Forward Selection: Search Procedure
p-value: Objective Function

### Forward, backward and stepwise selection

Forward selection is above.

Stepwise is similar, but after a predictor is chosen, the other chosen
predictors are re-evaluated for removal.

Backward selection starts with all predictors and removes one at a time

### Simulated Annealing

Selects a random subset of predictors and evaluates model, then selects
another random subset and evaulates model. This continues over a range
of iterations. The best iteration is not always selected. Sometimes, a
worse iteration is selected in order to avoid local maxima in the data
and instead fit a more general best fit.

## Genetic Algos

Works like chromosomes and genes. A model is attempted with a random
subset of predictors. Another iteration is attempted with some
predictors crossed-over and/or mutated.

A tuning paramter Pm, is set as the mutation parameter. High values
avoid local maxima but increase computation.

## Filter Methods

Dangerous due to applying inferential statistical tests. Bonferroni
correction can be applied to help correct this danger, but danger still
exists

## Selection Bias

The Modeler must be very careful to avoid selection bias. Situations
where selection bias occurs is especially true when:

+ small data set
+ P to n ratio is high since probability of incorrectly identifying
  non-informative predictor as informative is high)
+ black box method like SVM / ANN is used
+ no independent test set is available


